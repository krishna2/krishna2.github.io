# ML Reading List

## EMBEDDINGS, WORD2VEC, TRANSFORMERS AND BERT

### Jay Alamar

The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time
http://jalammar.github.io/illustrated-word2vec/

A Visual and Interactive Guide to the Basics of Neural Networks – Jay Alammar – Visualizing machine learning one concept at a time
http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/

A Visual And Interactive Look at Basic Neural Network Math – Jay Alammar – Visualizing machine learning one concept at a time
http://jalammar.github.io/feedforward-neural-networks-visual-interactive/

Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time
http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/

The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time
http://jalammar.github.io/illustrated-transformer/

The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time
http://jalammar.github.io/illustrated-gpt2/

The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time
http://jalammar.github.io/illustrated-bert/



### Chris Olah

Understanding Convolutions - colah's blog
http://colah.github.io/posts/2014-07-Understanding-Convolutions/imagenet.pdf

Deep Learning, NLP, and Representations - colah's blog
(this post has lots of very interesting ideas)
http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/

Understanding LSTM Networks -- colah's blog
http://colah.github.io/posts/2015-08-Understanding-LSTMs/

https://distill.pub/2016/augmented-rnns/

Neural Networks, Types, and Functional Programming -- colah's blog
http://colah.github.io/posts/2015-09-NN-Types-FP/


### Sebastian Ruder

On word embeddings - Part 1
https://ruder.io/word-embeddings-1/index.html

Approximating the Softmax for Learning Word Embeddings
https://ruder.io/word-embeddings-softmax/

On word embeddings - Part 3: The secret ingredients of word2vec
https://ruder.io/secret-word2vec/index.html

A survey of cross-lingual word embedding models
https://ruder.io/cross-lingual-embeddings/index.html

Word embeddings in 2017: Trends and future directions
https://ruder.io/word-embeddings-2017/index.html

https://ruder.io/deep-learning-nlp-best-practices/index.html#attention

http://ruder.io/state-of-transfer-learning-in-nlp/


### Chris McCormick

Chris McCormick tutorials / links:
About · Chris McCormick
https://mccormickml.com/about/

Chris McCormick ML
https://www.chrismccormick.ai/offers/GFUU2mEb/checkout

BERT Word Embeddings Tutorial · Chris McCormick
https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/

BERT Fine-Tuning Tutorial with PyTorch · Chris McCormick
https://mccormickml.com/2019/07/22/BERT-fine-tuning/

Tutorials · Chris McCormick
https://mccormickml.com/tutorials/

Word2Vec Tutorial - The Skip-Gram Model · Chris McCormick
http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

Word2Vec Tutorial Part 2 - Negative Sampling · Chris McCormick
http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/

Word2Vec Resources · Chris McCormick
http://mccormickml.com/2016/04/27/word2vec-resources/

https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/


### Annotated Papers and Concepts

The Annotated Transformer
https://nlp.seas.harvard.edu/2018/04/03/attention.html

The Annotated Encoder Decoder | A PyTorch tutorial implementing Bahdanau et al. (2015)
https://bastings.github.io/annotated_encoder_decoder/


### Temp : check out first

Copy of BERT_lab.ipynb - Colaboratory
https://colab.research.google.com/drive/1-UdelAqtD_2KrUIV_NTL59kgqo8m1Y2H#scrollTo=lJLcUBF5GOyU

BERT Explained: A Complete Guide with Theory and Tutorial – Towards Machine Learning
https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/

https://medium.com/swlh/transfer-learning-in-nlp-f5035cc3f62f

https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3

https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b

https://towardsdatascience.com/deconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1

https://towardsdatascience.com/bert-classifier-just-another-pytorch-model-881b3cf05784


https://github.com/bollu/bollu.github.io#everything-you-know-about-word2vec-is-wrong

http://www.peterbloem.nl/blog/transformers

https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45

https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3

https://towardsdatascience.com/illustrated-efficient-neural-architecture-search-5f7387f9fb6

https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21

https://towardsdatascience.com/visualisation-of-embedding-relations-word2vec-bert-64d695b7f36

https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html

https://towardsdatascience.com/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3



### Others

Learn Word2Vec by implementing it in tensorflow - Towards Data Science
https://towardsdatascience.com/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac

How to Develop Word Embeddings in Python with Gensim
https://machinelearningmastery.com/develop-word-embeddings-python-gensim/

Deep Learning For Natural Language Processing
https://machinelearningmastery.com/deep-learning-for-nlp/

A Brief History of Word Embeddings | Gavagai
https://www.gavagai.io/text-analytics/a-brief-history-of-word-embeddings/

The Unreasonable Effectiveness of Recurrent Neural Networks
http://karpathy.github.io/2015/05/21/rnn-effectiveness/

Attention, CNN and what not for Text Classification
https://towardsdatascience.com/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566


Google AI Blog: Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing
https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html


Google AI Blog: Inceptionism: Going Deeper into Neural Networks
https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html

Google AI Blog: Highlights from the 3rd Cohort of the Google AI Residency Program
https://ai.googleblog.com/2019/11/highlights-from-2019-google-ai.html



[1910.10683] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
https://arxiv.org/abs/1910.10683

[1911.11423] Single Headed Attention RNN: Stop Thinking With Your Head
https://arxiv.org/abs/1911.11423

1910.11664.pdf
https://arxiv.org/pdf/1910.11664.pdf

[1506.02078] Visualizing and Understanding Recurrent Networks
https://arxiv.org/abs/1506.02078


Exploring the Limits of Language Modeling
https://arxiv.org/pdf/1602.02410.pdf


Text Understanding from Scratch
https://arxiv.org/pdf/1502.01710v5.pdf



https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b


https://searchengineland.com/a-deep-dive-into-bert-how-bert-launched-a-rocket-into-natural-language-understanding-324522/amp

https://towardsdatascience.com/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac

https://machinelearningmastery.com/develop-word-embeddings-python-gensim/

https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9

https://www.saama.com/blog/attention-mechanism-benefits-and-applications/

https://explosion.ai/blog/deep-learning-formula-nlp



https://www.searchenginejournal.com/bert-generate-meta-descriptions/332585/amp/

https://blog.scaleway.com/2019/understanding-text-with-bert/amp/

https://towardsdatascience.com/understanding-bert-is-it-a-game-changer-in-nlp-7cca943cf3ad

https://israelg99.github.io/2017-03-23-Word2Vec-Explained/

https://www.wired.com/story/ai-pioneer-algorithms-understand-why/amp

https://medium.com/apache-mxnet/gluon-nlp-bert-6a489bdd3340

http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html

https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d

https://blog.scaleway.com/2019/building-a-machine-reading-comprehension-system-using-the-latest-advances-in-deep-learning-for-nlp/


https://blog.floydhub.com/gpt2/

https://transformer.huggingface.co/

https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281

https://towardsdatascience.com/neural-networks-and-philosophy-of-language-31c34c0796da

https://allennlp.org/elmo

https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html


Attention and Augmented Recurrent Neural Networks

https://distill.pub/2016/augmented-rnns/

https://austingwalters.com/word-embedding-and-data-splitting/

https://www.kdnuggets.com/2019/01/burkov-self-supervised-learning-word-embeddings.html

https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/

https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2

https://towardsdatascience.com/creating-word-embeddings-for-out-of-vocabulary-oov-words-such-as-singlish-3fe33083d466

Attention and Augmented Recurrent Neural Networks

https://distill.pub/2016/augmented-rnns/

ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators | OpenReview

https://openreview.net/forum?id=r1xMH1BtvB


-----

### PAPERS WITH CODE

Papers With Code : ALBERT: A Lite BERT for Self-supervised Learning of Language Representations

https://paperswithcode.com/paper/albert-a-lite-bert-for-self-supervised

Papers With Code : Language Models as Knowledge Bases?

https://paperswithcode.com/paper/language-models-as-knowledge-bases

Papers With Code : Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks

https://paperswithcode.com/paper/sentence-bert-sentence-embeddings-using

huggingface/transformers: 🤗 Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch.

https://github.com/huggingface/transformers

Papers With Code : Improved Knowledge Distillation via Teacher Assistant

https://paperswithcode.com/paper/improved-knowledge-distillation-via-teacher

Papers With Code : Plug and Play Language Models: A Simple Approach to Controlled Text Generation

https://paperswithcode.com/paper/plug-and-play-language-models-a-simple

Papers With Code : wav2vec: Unsupervised Pre-training for Speech Recognition

https://paperswithcode.com/paper/wav2vec-unsupervised-pre-training-for-speech

Papers With Code : Large Batch Optimization for Deep Learning: Training BERT in 76 minutes

https://paperswithcode.com/paper/reducing-bert-pre-training-time-from-3-days

Papers With Code : deepsing: Generating Sentiment-aware Visual Stories using Cross-modal Music Translation

https://paperswithcode.com/paper/deepsing-generating-sentiment-aware-visual

graykode/nlp-tutorial: Natural Language Processing Tutorial for Deep Learning Researchers

https://github.com/graykode/nlp-tutorial

Papers With Code : SpanBERT: Improving Pre-training by Representing and Predicting Spans

https://paperswithcode.com/paper/spanbert-improving-pre-training-by

Papers With Code : Star-Transformer

https://paperswithcode.com/paper/star-transformer


------

### CONFERENCES


https://iclr.cc/

https://icml.cc/

-----


### EMBEDDINGS : OTHER USE CASES


Fantastic talk (transcript) by Jay Allamar on Word2Vec and Embeddings. The talk mentions recommendations embedding at Airbnb, Alibaba, Asos, Spotify etc.Here are some links to read:
Intuition & Use-Cases of Embeddings in NLP & beyond

https://www.infoq.com/presentations/nlp-word-embedding/


Listing Embeddings in Search Ranking - Airbnb Engineering & Data Science - Medium

https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e

Real-time Personalization using Embeddings for Search Ranking at Airbnb

https://astro.temple.edu/~tua95067/kdd2018.pdf

Applying word2vec to Recommenders and Advertising · Chris McCormick

https://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/

Collaborative Embeddings for Lipstick Recommendations

https://towardsdatascience.com/collaborative-embeddings-for-lipstick-recommendations-98eccfa816bd

Applying Deep Learning To Airbnb Search

https://arxiv.org/pdf/1810.09591.pdf

Real-time Personalization using Embeddings for Search Ranking at Airbnb - Semantic Scholar

https://www.semanticscholar.org/paper/Real-time-Personalization-using-Embeddings-for-at-Grbovic-Cheng/2553596e3b24444d309ae461da50370d58f1b807

[1803.02349] Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba

https://arxiv.org/abs/1803.02349

[1905.06874] Behavior Sequence Transformer for E-commerce Recommendation in Alibaba

https://arxiv.org/abs/1905.06874


https://bytes.grubhub.com/search-query-embeddings-using-query2vec-f5931df27d79?gi=808bf4e3e025


Order Matters: Alibaba’s Transformer-based Recommender System

https://www.kdnuggets.com/2019/08/order-matters-alibabas-transformer-based-recommender-system.html

Finding the Right Clique: Alibaba Advances Exact-K Recommendation

https://medium.com/@alitech_2017/finding-the-right-clique-alibaba-advances-exact-k-recommendation-a3e88d23c297


Customer Lifetime Value Prediction Using Embeddings

http://arxiv-export-lb.library.cornell.edu/pdf/1703.02596

Customer Lifetime Value Prediction Using Embeddings

https://medium.com/syncedreview/customer-lifetime-value-prediction-using-embeddings-53f54e2ac59d

Fashion Forward – using deep learning at ASOS to predict customer lifetime value in online fashion retail – Technology and Operations Management

https://digital.hbs.edu/platform-rctom/submission/fashion-forward-using-deep-learning-at-asos-to-predict-customer-lifetime-value-in-online-fashion-retail/


How do ASOS use deep learning to optimise performance?

http://blog.re-work.co/how-do-asos-use-deep-learning-to-optimise-performance/

Table 2 from Customer Life Time Value Prediction Using Embeddings - Semantic Scholar

https://www.semanticscholar.org/paper/Customer-Life-Time-Value-Prediction-Using-Chamberlain-Cardoso/ebb7913b5c5bf1ce3b8aef8d0a4d2f0a704ba7a2/figure/3


Case Study: Music Recommendations at Spotify - MLRecipies - Medium

https://medium.com/mlrecipies/case-study-music-recommendations-at-spotify-3daf9d9f58c6

Recommending music on Spotify with deep learning – Sander Dieleman

http://benanne.github.io/2014/08/05/spotify-cnns.html

Building Music Playlists Recommendation System - Towards Data Science

https://towardsdatascience.com/building-music-playlists-recommendation-system-564a3e63ef64


How to discover new music on Spotify with Artificial Intelligence

https://towardsdatascience.com/how-to-discover-new-music-on-spotify-with-artificial-intelligence-b2110af6a611




-----

### NEURAL NETWORKS

neural networks non linearity - Google Search

https://www.google.com/search?q=neural+networks+non+linearity&oq=neural+networks+non-lin&aqs=chrome.2.69i57j0l5.7420j0j1&sourceid=chrome&ie=UTF-8


Neural networks and deep learning

http://neuralnetworksanddeeplearning.com/chap4.html


A Neural Network Playground

http://playground.tensorflow.org/#activation=linear&batchSize=20&dataset=circle&regDataset=reg-gauss&learningRate=0.003&regularizationRate=0.001&noise=0&networkShape=4,2&seed=0.87118&showTestData=false&discretize=true&percTrainData=80&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=regression&initZero=false&hideText=false


A Neural Network Playground

http://playground.tensorflow.org/#activation=relu&batchSize=20&dataset=circle&regDataset=reg-gauss&learningRate=0.003&regularizationRate=0.001&noise=0&networkShape=4,2&seed=0.97957&showTestData=false&discretize=true&percTrainData=80&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=regression&initZero=false&hideText=false


Visualizing The Non-linearity of Neural Networks - Towards Data Science

https://towardsdatascience.com/visualizing-the-non-linearity-of-neural-networks-c55b2a14ad7a


Why do you need non-linear activation functions? - Shallow neural networks | Coursera

https://www.coursera.org/lecture/neural-networks-deep-learning/why-do-you-need-non-linear-activation-functions-OASKH


what makes neural networks a nonlinear classification model? - Cross Validated

https://stats.stackexchange.com/questions/222639/what-makes-neural-networks-a-nonlinear-classification-model/222642

https://arxiv.org/pdf/1905.12337.pdf


How neural networks learn nonlinear functions and classify linearly non-separable data?

https://medium.com/@vivek.yadav/how-neural-networks-learn-nonlinear-functions-and-classify-linearly-non-separable-data-22328e7e5be1


Hacker's guide to Neural Networks

http://karpathy.github.io/neuralnets/


-----

### CNN

Conv Nets: A Modular Perspective - colah's blog
http://colah.github.io/posts/2014-07-Conv-Nets-Modular/

Understanding Convolutions - colah's blog
http://colah.github.io/posts/2014-07-Understanding-Convolutions/

Groups & Group Convolutions - colah's blog
http://colah.github.io/posts/2014-12-Groups-Convolution/

Visualizing MNIST: An Exploration of Dimensionality Reduction - colah's blog
http://colah.github.io/posts/2014-10-Visualizing-MNIST/

Visualizing Representations: Deep Learning and Human Beings - colah's blog
http://colah.github.io/posts/2015-01-Visualizing-Representations/


Simple Introduction to Convolutional Neural Networks
https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac

Illustrated: 10 CNN Architectures - Towards Data Science
https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d

An intuitive guide to Convolutional Neural Networks
https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/

The best explanation of Convolutional Neural Networks on the Internet!
https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8

An Intuitive Explanation of Convolutional Neural Networks – the data science blog
https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/

CS231n Convolutional Neural Networks for Visual Recognition
http://cs231n.github.io/neural-networks-1/

https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/

https://towardsdatascience.com/understanding-convolutional-neural-networks-221930904a8e


https://towardsdatascience.com/a-guide-to-convolutional-neural-networks-from-scratch-f1e3bfc3e2de

https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac


https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d

https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/

https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8

https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/



-----

### RNN / LSTM


Illustrated Guide to Recurrent Neural Networks - Towards Data Science
https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9

Illustrated Guide to Recurrent Neural Networks: Understanding the Intuition

https://www.youtube.com/watch?v=LHXXI4-IEns

Illustrated Guide to LSTM's and GRU's: A step by step explanation

https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21

https://www.youtube.com/watch?v=8HyCNIVRbSU


https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45


Illustrated Guide to Recurrent Neural Networks
http://kurious.pub/blog/4

The Basics Of Recurrent Neural Networks (RNN) | Built In
https://builtin.com/data-science/recurrent-neural-networks-and-lstm

The Beginner’s Guide to Recurrent Neural Networks and Text Generation
https://medium.com/@annikabrundyn1/the-beginners-guide-to-recurrent-neural-networks-and-text-generation-44a70c34067f

A Beginner's Guide to LSTMs and Recurrent Neural Networks | Skymind
https://skymind.ai/wiki/lstm

karpathy/char-rnn: Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch
https://github.com/karpathy/char-rnn



-----


## COURSES, PAPERS AND BOOKS


https://web.stanford.edu/~jurafsky/slp3/

https://www.preview.nearist.ai/paid-ebook-and-tutorial


DeepLizard : seems to have a full playlist of three ml/dl courses. Here are the top 4 that is of interest.

Machine Learning & Deep Learning Fundamentals

https://deeplizard.com/learn/playlist/PLZbbT5o_s2xq7LwI2y8_QtvuXZedL6tQU

Keras - Python Deep Learning Neural Network API

https://deeplizard.com/learn/playlist/PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL

Neural Network Programming - Deep Learning with PyTorch

https://deeplizard.com/learn/playlist/PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG

Reinforcement Learning - Introducing Goal Oriented Intelligence

https://deeplizard.com/learn/playlist/PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv


Convolutional Neural Networks (CNN) - Deep Learning Wizard
https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/

Neural networks [10.7] : Natural language processing - hierarchical output layer - YouTube
https://www.youtube.com/watch?v=B95LTf2rVWM

Neural networks [1.1] : Feedforward neural network - artificial neuron - YouTube
https://www.youtube.com/watch?v=SGZ6BttHMPw&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH

A friendly introduction to Recurrent Neural Networks - YouTube
https://www.youtube.com/watch?v=UNmqTiOnRfg

Home - Keras Documentation
https://keras.io/#getting-started-30-seconds-to-keras


keras/examples at master · keras-team/keras
https://github.com/keras-team/keras/tree/master/examples


keras/lstm_text_generation.py at master · keras-team/keras
https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py


Difference between view, reshape, transpose and permute in PyTorch - jdhao's blog
https://jdhao.github.io/2019/07/10/pytorch_view_reshape_transpose_permute/


neural network - PyTorch - contiguous() - Stack Overflow
https://stackoverflow.com/questions/48915810/pytorch-contiguous


PyTorch: Custom nn Modules — PyTorch Tutorials 1.3.0 documentation
https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html


Deep Learning with PyTorch: A 60 Minute Blitz — PyTorch Tutorials 1.3.0 documentation
https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html


NLP From Scratch: Translation with a Sequence to Sequence Network and Attention — PyTorch Tutorials 1.3.0 documentation
https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html

Learning PyTorch with Examples — PyTorch Tutorials 1.3.0 documentation
https://pytorch.org/tutorials/beginner/pytorch_with_examples.html

Language Translation with TorchText — PyTorch Tutorials 1.3.0 documentation
https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html

torchaudio Tutorial — PyTorch Tutorials 1.3.0 documentation
https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html


allennlp/cnn_encoder.py at master · allenai/allennlp
https://github.com/allenai/allennlp/blob/master/allennlp/modules/seq2vec_encoders/cnn_encoder.py

bamtercelboo/pytorch_Highway_Networks: Highway Networks implement in pytorch
https://github.com/bamtercelboo/pytorch_Highway_Networks

Implementing a CNN for Text Classification in TensorFlow – WildML
http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/

Your First Deep Learning Project in Python with Keras Step-By-Step
https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/

Deep Learning with Python
http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf


https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb


Weights and Biases : wandb.com : This is by Lukas Biewalk (figure eight company): The articles and tutorials seem to cover very important material - worth a look:

https://www.wandb.com/articles

https://www.wandb.com/tutorials



CNN, RNN, LSTM, Word2Vec, Transformer : Tutorials, intuitive explanations.

https://stackoverflow.com/questions/57048120/pytorch-lstm-vs-lstmcell

https://stackoverflow.com/questions/48187283/whats-the-difference-between-lstm-and-lstmcell

https://colab.research.google.com/drive/1-UdelAqtD_2KrUIV_NTL59kgqo8m1Y2H#scrollTo=lJLcUBF5GOyU

https://towardsdatascience.com/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566

https://www.kaggle.com/mlwhiz/textcnn-pytorch-and-keras

https://github.com/cmasch/cnn-text-classification

https://scholar.google.com/citations?user=32w7x1cAAAAJ&hl=en


https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5

https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62


https://machinelearningmastery.com/deep-learning-for-nlp/

https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/


Math for Machine Learning book:

https://mml-book.github.io/book/mml-book.pdf

Deep Learning with PyTorch:

https://news.ycombinator.com/item?id=21240057


Lots of ML: https://blog.floydhub.com/


https://www.coursera.org/specializations/tensorflow-in-practice

https://www.coursera.org/learn/introduction-tensorflow/home/welcome

https://www.coursera.org/learn/convolutional-neural-networks-tensorflow/home/welcome

https://www.coursera.org/learn/natural-language-processing-tensorflow/home/welcome

https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/home/welcome

https://www.coursera.org/learn/language-processing

https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/home/welcome

https://www.coursera.org/learn/guided-tour-machine-learning-finance/home/welcome

https://www.coursera.org/learn/fundamentals-machine-learning-in-finance/home/welcome

https://www.coursera.org/learn/reinforcement-learning-in-finance/home/welcome

https://www.coursera.org/learn/advanced-methods-reinforcement-learning-finance/home/welcome


https://www.coursera.org/specializations/tensorflow-in-practice

https://www.coursera.org/specializations/aml

https://www.coursera.org/specializations/machine-learning-reinforcement-finance

https://www.coursera.org/specializations/tensorflow-in-practice

Couple of course links from Stanford NLP + DL :

http://web.stanford.edu/class/cs224n/

https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/syllabus.html

http://web.stanford.edu/class/cs224u/

Current top tabs for learning:

https://course.fast.ai/videos/?lesson=1

http://course18.fast.ai/part2.html

https://www.fast.ai/2018/04/29/categorical-embeddings/

https://www.fast.ai/2018/07/12/auto-ml-1/

https://www.fast.ai/2018/07/16/auto-ml2/

https://www.fast.ai/2018/07/23/auto-ml-3/

http://nlp.fast.ai/


CMU LTI NLP course:

http://phontron.com/class/nn4nlp2019/schedule.html

Very good book on Theoretical Computer Science:

https://introtcs.org/public/lec_13_Cook_Levin.html

The chapter on p, np, ..etc is good - I should use this as model for simplified explanation for savit.


A free book on Reinforcement Learning:
https://arxiv.org/pdf/1811.12560.pdf


Looks like fast.ai is the best way to go about this. They have a new V3 of their course completely redone.

https://news.ycombinator.com/item?id=19000027

https://www.fast.ai/2019/01/24/course-v3/

https://course.fast.ai/

http://course18.fast.ai/part2.html

https://explained.ai/matrix-calculus/index.html


https://sites.google.com/view/deep-rl-bootcamp/lectures


UC Berkeley's Deep Learning course with sections on CNN / RNN / NLP and Word Embeddings:

http://d2l.ai/chapter_natural-language-processing/index.html


https://www.coursera.org/learn/language-processing/home/welcome


### PEOPLE

Andrej Karpathy Academic Website
https://cs.stanford.edu/people/karpathy/

Sebastian Ruder
https://ruder.io/

HackerNoon Interview
https://ruder.io/hackernoon-interview/

Ronan Collobert - Google Scholar Citations
https://scholar.google.com/citations?user=32w7x1cAAAAJ&hl=en

Michael Nielsen

http://michaelnielsen.org/

Fernanda Viégas

http://www.fernandaviegas.com/


David Silver (computer scientist) - Wikipedia
https://en.wikipedia.org/wiki/David_Silver_(computer_scientist)


David Silver - Google Scholar Citations
https://scholar.google.com/citations?user=-8DNE4UAAAAJ&hl=en


https://ai.google/research/people/author37567/


-----

## MISC

GitHub - trekhleb/nano-neuron: NanoNeuron is 7 simple JavaScript functions that will give you a feeling of how machines can actually "learn"
https://github.com/trekhleb/nano-neuron

Show HN: AI Dungeon 2 – AI-generated text adventure built with 1.5B param GPT-2 | Hacker News
https://news.ycombinator.com/item?id=21717022

The unending quest to explain consciousness | Hacker News
https://news.ycombinator.com/item?id=21706591

Making a text adventure game with GPT2 | Hacker News
https://news.ycombinator.com/item?id=21711808

1911.05289.pdf
https://arxiv.org/pdf/1911.05289.pdf

1701.06538.pdf
https://arxiv.org/pdf/1701.06538.pdf

How to apply machine learning and deep learning methods to audio analysis
https://medium.com/comet-ml/applyingmachinelearningtoaudioanalysis-utm-source-kdnuggets11-19-e160b069e88

[1911.08265] Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model
https://arxiv.org/abs/1911.08265


-----

### FINANCE

https://www.tradientblog.com/2019/11/lessons-learned-building-an-ml-trading-system-that-turned-5k-into-200k/
https://news.ycombinator.com/item?id=21647038

https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos-ebook-dp-B079KLDW21/dp/B079KLDW21/ref=mt_kindle?_encoding=UTF8&me=&qid=
https://arxiv.org/list/q-fin/recent

Deep Learning for Time Series Forecasting
https://machinelearningmastery.com/deep-learning-for-time-series-forecasting/

https://arxiv.org/abs/1911.12540

https://www.amazon.com/Python-Finance-Mastering-Data-Driven-ebook/dp/B07L8NMW2P/


-----

### MUSIC / LANGUAGE / GAME

Magenta
https://magenta.tensorflow.org/


Generating Piano Music with Transformer
https://magenta.tensorflow.org/piano-transformer


Magenta Studio
https://magenta.tensorflow.org/studio


Google Magenta-Making Music with MIDI and Machine Learning -
https://www.midi.org/articles-old/google-magenta-making-music-with-midi-and-machine-learning

Behind Magenta, the tech that rocked I/O
https://blog.google/technology/ai/behind-magenta-tech-rocked-io/

https://www.ethanhein.com/wp/2016/visualizing-hip-hop-melodies/


Text Generation With LSTM Recurrent Neural Networks in Python with Keras
https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/

Some interesting articles on language / nlp:

http://nautil.us/issue/54/the-unspoken/the-rise-and-fall-of-the-english-sentence

https://hackernoon.com/generating-lyrics-using-deep-multi-layer-lstm-b28ee8124936

https://towardsdatascience.com/ai-generates-taylor-swifts-song-lyrics-6fd92a03ef7e

http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/


https://magenta.tensorflow.org/piano-transformer

https://magenta.tensorflow.org/studio

https://www.midi.org/articles-old/google-magenta-making-music-with-midi-and-machine-learning

https://blog.google/technology/ai/behind-magenta-tech-rocked-io/


-----

### MATH

Terence Parr : Matrics Calculus for DL:

https://explained.ai/matrix-calculus/index.html

https://explained.ai/

Calculus explained with GIFs:

https://news.ycombinator.com/item?id=21671112

Calculus: https://0a.io/chapter1/calculus-explained.html

Jim Fowler Calculus: https://www.youtube.com/channel/UCt1n_c_lbPIvz_ycw3Eq96w


-----


### WIKI LINKS AND TERMINOLOGY

Receiver operating characteristic - Wikipedia
https://en.wikipedia.org/wiki/Receiver_operating_characteristic


Understanding AUC - ROC Curve - Towards Data Science
https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5


Understanding Confusion Matrix - Towards Data Science
https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62



-----


### MORE MISC : OR IT IS YET TO BE CLASSIFIED


Trouble in ML:

https://arxiv.org/abs/1807.03341

Hofstader on Google Translate:

https://www.theatlantic.com/technology/archive/2018/01/the-shallowness-of-google-translate/551570/


TextCNN - Pytorch and Keras | Kaggle
https://www.kaggle.com/mlwhiz/textcnn-pytorch-and-keras

cmasch/cnn-text-classification: Text classification with Convolution Neural Networks on Yelp, IMDB & sentence polarity dataset v1.0
https://github.com/cmasch/cnn-text-classification



Thread by @seb_ruder: "David Silver on Principles for Reinforcement Learning at the . Important principles that are not only applicable to RL, but to […]" #DLIndaba2018

https://threadreaderapp.com/thread/1040235236284669952.html

10 Exciting Ideas of 2018 in NLP: https://ruder.io/10-exciting-ideas-of-2018-in-nlp/


Requests for Research: https://ruder.io/requests-for-research/


1909.04547.pdf: https://arxiv.org/pdf/1909.04547.pdf



tensorflow/tensor2tensor: Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research.
https://github.com/tensorflow/tensor2tensor



Deconvolution and Checkerboard Artifacts
https://distill.pub/2016/deconv-checkerboard/



Fast Markov chains in ~20 lines of sh, grep, cut and Awk | Hacker News
https://news.ycombinator.com/item?id=21493761

Naïve Bayes for Machine Learning – From Zero to Hero | Hacker News
https://news.ycombinator.com/item?id=21537314

NLP's ImageNet moment has arrived
https://thegradient.pub/nlp-imagenet/


tensorflow/nmt: TensorFlow Neural Machine Translation Tutorial
https://github.com/tensorflow/nmt


Very good text generation example: (in keras. trains with Nietzsche's text):

https://keras.io/examples/lstm_text_generation/


Amazing visualization and analysis of every Nature paper published and their connections.

https://www.youtube.com/watch?v=GW4s58u8PZo&feature=youtu.be

https://www.nature.com/immersive/d41586-019-03165-4/index.html

https://www.nature.com/immersive/d41586-019-03165-4/reftree-home.html


Wonder if such a history exists for ML, NLP, esp. Music/ML.

MaxtonChangeJulv1.pdf
http://www.graememaxton.com/admin/resources/MaxtonChangeJulv1.pdf


Neural Network Architectures - Towards Data Science
https://towardsdatascience.com/neural-network-architectures-156e5bad51ba

[1606.04474] Learning to learn by gradient descent by gradient descent
https://arxiv.org/abs/1606.04474

Neuroevolution of augmenting topologies - Wikipedia
https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies

404 Not Found
http://people.idsia.ch/~daan/papers/gomez-ijcai05.pdf

NeuroEvolution of Augmenting Topologies
http://www.cs.ucf.edu/~kstanley/neat.html

Evolving Deep Neural Networks
https://arxiv.org/pdf/1703.00548.pdf

[1703.01041] Large-Scale Evolution of Image Classifiers
https://arxiv.org/abs/1703.01041

1606.02580.pdf
https://arxiv.org/pdf/1606.02580.pdf


Amazing site: NLP Explorer : http://104.154.102.7/(sadly yes, it doesn't have a web site name.. :) ).
http://104.154.102.7/papers_all


And it is itself described in a paper: https://arxiv.org/abs/1910.07351


https://medium.com/@rafayak/nothing-but-numpy-understanding-creating-binary-classification-neural-networks-with-e746423c8d5c



https://arxiv.org/pdf/1911.01547.pdf
Francois Chollet's on "Need for an actionable definition and measure of intelligence".

MaxtonChangeJulv1.pdf
http://www.graememaxton.com/admin/resources/MaxtonChangeJulv1.pdf




1901.07291.pdf
https://arxiv.org/pdf/1901.07291.pdf#page9


1911.02116.pdf
https://arxiv.org/pdf/1911.02116.pdf


Character-Aware Neural Language Models
https://arxiv.org/pdf/1508.06615.pdf


Highway Networks
https://arxiv.org/pdf/1505.00387.pdf

Advanced NLP Course to do on Coursera - krishna2@gmail.com - Gmail
https://mail.google.com/mail/u/0/#inbox/QgrcJHsbjCmCVqsXCdwGkpKZvNSGPxXkNkv

Drawing tools on web but makes it look like hand-drawn:

https://sketchviz.com/graphviz-examples

https://github.com/jgraph/drawio




https://www.blog.google/technology/ai/teachable-machine/

https://michelenasti.com/2019/10/21/how-internet-ads-work.html



The magic of generating new ideas | Hacker News

https://news.ycombinator.com/item?id=21513790

Social Media Is Warping Democracy - The Atlantic

https://www.theatlantic.com/magazine/archive/2019/12/social-media-democracy/600763/

Open vs Closed pipes (Flutes vs Clarinets)

http://newt.phys.unsw.edu.au/jw/flutes.v.clarinets.html

GPT-2 Neural Network Poetry - Gwern.net

https://www.gwern.net/GPT-2

Optima Optometry | Optometrist | Eye Doctor | Eye Care | Eyeglasses | Contact Lenses | Santa Clara CA

https://www.optimaoptometry.com/


Visualizing memorization in RNNs

https://distill.pub/2019/memorization-in-rnns/

Learning to Predict Without Looking Ahead: World Models Without Forward Prediction

https://learningtopredict.github.io/


http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf

https://towardsdatascience.com/demystifying-convolutional-neural-networks-384785791596

https://futurism.com/redditor-claims-love-ai-gpt-2/amp

https://thegradient.pub/nlps-clever-hans-moment-has-arrived/


https://news.ycombinator.com/item?id=21493761

https://machinelearningmastery.com/deep-learning-for-time-series-forecasting/

http://www.incompleteideas.net/IncIdeas/BitterLesson.html

https://rodneybrooks.com/a-better-lesson/

https://www.reddit.com/r/MachineLearning/comments/ds1xvc/d_deep_learning_has_a_size_problem_we_need_to/f6nnq8d/

https://ai.googleblog.com/2019/11/highlights-from-2019-google-ai.html


http://kurious.pub/blog/4

https://builtin.com/data-science/recurrent-neural-networks-and-lstm

https://medium.com/@annikabrundyn1/the-beginners-guide-to-recurrent-neural-networks-and-text-generation-44a70c34067f

https://skymind.ai/wiki/lstm

https://en.wikipedia.org/wiki/Universal_approximation_theorem

https://heartbeat.fritz.ai/k-means-clustering-using-sklearn-and-python-4a054d67b187

https://arxiv.org/pdf/1409.0473.pdf

https://arxiv.org/pdf/1902.02181.pdf

https://arxiv.org/pdf/1703.03906.pdf

https://www.kalzumeus.com/2019/10/28/tether-and-bitfinex/

https://www.technologyreview.com/s/614666/ai-machine-learning-music-feel-good/amp/

https://towardsdatascience.com/beginning-machine-learning-650add627e79

https://www.sciencealert.com/prime-number-theorems-conjectures-explained/amp

https://medium.com/tensorflow/using-tensorflow-2-for-state-of-the-art-natural-language-processing-102445cda54a

http://www.bbc.com/culture/story/20191022-what-are-the-best-first-lines-in-fiction

https://marketingland.com/welcome-bert-googles-latest-search-algorithm-to-better-understand-natural-language-269910/amp

https://www.sciencealert.com/mathematicians-have-discovered-an-astonishing-new-way-to-multiply-large-numbers/amp

https://aiweirdness.com/post/188342947482/halloween-costumes-by-the-neural-net-gpt-2/amp

https://towardsdatascience.com/teaching-a-neural-net-to-play-blackjack-8ec5f39809e2

https://towardsdatascience.com/tensorflow-2-0-create-and-train-a-vanilla-cnn-on-google-colab-c7a0ac86d61b

https://www.apptic.me/blog/how-to-train-a-neural-net-to-play-cards.php

https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456

https://appliedmachinelearning.blog/2019/03/04/state-of-the-art-text-classification-using-bert-model-predict-the-happiness-
hackerearth-challenge/amp/

https://classicalpoets.org/2018/05/24/10-favorite-shakespeare-sonnets/

https://towardsdatascience.com/fse-2b1ffa791cf9

https://github.com/tensorflow/text

https://medium.com/syncedreview/going-beyond-gan-new-deepmind-vae-model-generates-high-fidelity-human-faces-b1cc08fa4bbb

https://www.pyimagesearch.com/2018/09/10/keras-tutorial-how-to-get-started-with-keras-deep-learning-and-python/

https://medium.com/datadriveninvestor/deep-learning-with-python-and-fast-ai-part-2-nlp-classification-with-transfer-learning-e7aaf7514e04

https://medium.com/@narjes.karmani/reinforcement-learning-fundamentals-469a91e40fce

https://news.ycombinator.com/item?id=21242343

https://www.newyorker.com/magazine/2019/10/14/can-a-machine-learn-to-write-for-the-new-yorker

http://www.oranlooney.com/post/ml-from-scratch-part-6-pca/


Deep Learning Reading List:

https://github.com/ChristosChristofidis/awesome-deep-learning

https://github.com/aikorea/awesome-rl#papers--thesis

http://web.archive.org/web/20170612030342/

http://karpathy.github.io/2016/09/07/phd/

https://spinningup.openai.com/en/latest/spinningup/keypapers.html




https://news.ycombinator.com/item?id=15938082


https://news.ycombinator.com/item?id=20773992

https://blog.usejournal.com/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc


https://blog.dominodatalab.com/deep-reinforcement-learning/

https://towardsdatascience.com/step-by-step-tutorial-on-linear-regression-with-stochastic-gradient-descent-1d35b088a843

https://news.ycombinator.com/item?id=20524543

https://victorzhou.com/blog/intro-to-rnns/


https://towardsdatascience.com/deep-learning-from-scratch-and-using-tensorflow-in-python-34aad75f939


Information theory for intelligent people:

http://tuvalu.santafe.edu/~simon/it.pdf

Shannon's entropy:

http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf

Visual information theory:

https://colah.github.io/posts/2015-09-Visual-Information/

Are we making progress:

https://arxiv.org/pdf/1907.06902v1.pdf

Statistical modeling : Two cultures:

http://www2.math.uu.se/~thulin/mm/breiman.pdf

Singapore's how to build good software:

https://www.csc.gov.sg/articles/how-to-build-good-software

https://news.ycombinator.com/item?id=20773992


https://medium.com/@vanya_cohen/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc

https://news.ycombinator.com/item?id=20771604


https://hackernoon.com/interview-with-deep-learning-researcher-and-leader-of-openmined-andrew-trask-77cd33570a8c

https://towardsdatascience.com/step-by-step-tutorial-on-linear-regression-with-stochastic-gradient-descent-1d35b088a843

This is the main article's (first link) hn post with good comments and more links:

https://news.ycombinator.com/item?id=19498356


The first one is new but uses pytorch. The second one is bit old but uses tensorflow.
https://monkeylearn.com/text-analysis/


Writing a Neural Network from scratch in Python:

https://victorzhou.com/blog/intro-to-neural-networks/


GANs:

https://medium.com/@jonathan_hui/gan-whats-generative-adversarial-networks-and-its-application-f39ed278ef09

https://medium.com/@jonathan_hui/gan-some-cool-applications-of-gans-4c9ecca35900

https://ai.googleblog.com/2018/08/introducing-new-framework-for-flexible.html

https://blog.openai.com/better-language-models/#

https://news.ycombinator.com/item?id=19163522



Some long pending C++ links:

https://boredzo.org/pointers/http://www.c-faq.com/aryptr/index.html

http://www.dietmar-kuehl.de/mirror/c++-faq/references.html

https://isocpp.org/faq

https://blog.openai.com/spinning-up-in-deep-rl/

https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a

https://austingwalters.com/neural-networks-to-production-from-an-engineer/


https://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/


And a slightly off-topic post by Sam Altman : How to be Successful:

http://blog.samaltman.com/how-to-be-successful

And a bit in to the C++ land:

https://quuxplusone.github.io/blog/2019/01/20/covariance-and-contravariance/

https://github.com/ssloy/tinyraytracer/wiki

Few more good links to read:


https://en.wikipedia.org/wiki/Distributional_semantics

https://aurelieherbelot.net/research/distributional-semantics-intro/



ML / NLP related articles and readings:


https://mlwhiz.com/blog/2018/12/17/text_classification/

https://mlwhiz.com/blog/2017/02/05/Machine_learning_algorithms_for_data_scientist/

https://mlwhiz.com/blog/2015/08/19/MCMC_Algorithms_Beta_Distribution/

https://mlwhiz.com/blog/2015/08/21/MCMC_Algorithms_Cryptography/


https://jeffhuang.com/best_paper_awards.html






