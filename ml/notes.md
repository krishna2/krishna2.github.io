# NOTES : RANDOM STUFF, LINKS

## 2020-01-14

Why I Keep a Research Blog
http://gregorygundersen.com/blog/2020/01/12/why-research-blog/

15848417.pdf
https://web.stanford.edu/class/cs224n/reports/default/15848417.pdf

huggingface/tokenizers: üí•Fast State-of-the-Art Tokenizers optimized for Research and Production
[https://github.com/huggingface/tokenizers](https://github.com/huggingface/tokenizers)

Free ‚ÄúLanguage Learning with Netflix‚Äù extension makes studying Japanese almost too¬†easy | SoraNews24 -Japan News-
[https://soranews24.com/2020/01/12/free-language-learning-with-netflix-extension-makes-studying-japanese-almost-too-easy/](https://soranews24.com/2020/01/12/free-language-learning-with-netflix-extension-makes-studying-japanese-almost-too-easy/)

jdevlin.pdf
[https://nlp.stanford.edu/seminar/details/jdevlin.pdf](https://nlp.stanford.edu/seminar/details/jdevlin.pdf)

Pre-training BERT from scratch with cloud TPU - Towards Data Science
[https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379](https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379)

Papers With Code : BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
[https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional](https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional)

BERT Fine-Tuning Tutorial with PyTorch ¬∑ Chris McCormick
[https://mccormickml.com/2019/07/22/BERT-fine-tuning/](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)

Fine-tuning Bert language model to get better results on text classification
[https://medium.com/analytics-vidhya/fine-tuning-bert-language-model-to-get-better-results-on-text-classification-3dac5e3c348e](https://medium.com/analytics-vidhya/fine-tuning-bert-language-model-to-get-better-results-on-text-classification-3dac5e3c348e)

BERT Explained: A Complete Guide with Theory and Tutorial ‚Äì Towards Machine Learning
[https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/)

BERT Technology introduced in 3-minutes ‚Äì mc.ai
[https://mc.ai/bert-technology-introduced-in-3-minutes/](https://mc.ai/bert-technology-introduced-in-3-minutes/)

Pretrained models ‚Äî transformers 2.3.0 documentation
[https://huggingface.co/transformers/pretrained_models.html](https://huggingface.co/transformers/pretrained_models.html)

Multi-lingual models ‚Äî transformers 2.3.0 documentation
[https://huggingface.co/transformers/multilingual.html](https://huggingface.co/transformers/multilingual.html)

Pre-training, Transformers, and Bi-directionality | AISC Blog
[https://aisc.ai.science/blog/2019/bert-pretraining-transformers-bidirectionality](https://aisc.ai.science/blog/2019/bert-pretraining-transformers-bidirectionality)

BERT Explained: State of the art language model for NLP
[https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)

BERT Technology introduced in 3-minutes - Towards Data Science
[https://towardsdatascience.com/bert-technology-introduced-in-3-minutes-2c2f9968268c](https://towardsdatascience.com/bert-technology-introduced-in-3-minutes-2c2f9968268c)

Paper Dissected: "Attention is All You Need" Explained | Machine Learning Explained
[http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/](http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/)

(155) Daniel Kahneman: Thinking Fast and Slow, Deep Learning, and AI | Artificial Intelligence Podcast - YouTube
[https://www.youtube.com/watch?v=UwwBG-MbniY&list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4](https://www.youtube.com/watch?v=UwwBG-MbniY&list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4)

Dissecting BERT Part 1: The Encoder - Dissecting BERT - Medium
[https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3](https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3)

Understanding BERT Part 2: BERT Specifics - Dissecting BERT - Medium
[https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73](https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73)

Dissecting BERT Appendix: The Decoder - Dissecting BERT - Medium
[https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f](https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f)

BERT in a Nutshell ‚Äì mc.ai
[https://mc.ai/bert-in-a-nutshell/](https://mc.ai/bert-in-a-nutshell/)

Understand Self-Attention in BERT Intuitively - Towards Data Science
[https://towardsdatascience.com/understand-self-attention-in-bert-intuitively-cd480cbff30b](https://towardsdatascience.com/understand-self-attention-in-bert-intuitively-cd480cbff30b)

Deconstructing BERT: Distilling 6 Patterns from 100 Million Parameters
[https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77](https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77)

Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention
[https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1)

3 subword algorithms help to improve your NLP model performance
[https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46](https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46)

What's the difference between wordpiece and sentencepiece?
[https://www.gitmemory.com/issue/google/sentencepiece/339/497563846](https://www.gitmemory.com/issue/google/sentencepiece/339/497563846)

A Deep Dive into Preprocessing in NLP | Machine Learning Explained
[https://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/](https://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/)

Learning to rank - Wikipedia
[https://en.wikipedia.org/wiki/Learning_to_rank](https://en.wikipedia.org/wiki/Learning_to_rank)

The first forum for newcomers to ML is co-located with NeurIPS, East Meeting Level 11,12, Vancouver Convention Center, Vancouver, BC, Canada, Monday, December 9th, 2019. | New In ML 2019
[https://nehzux.github.io/NewInML2019/](https://nehzux.github.io/NewInML2019/)

Nine things I wish I had known the first time I came to NIPS
[https://medium.com/@jennwv/nine-things-i-wish-i-had-known-the-first-time-i-came-to-nips-b939330661ed](https://medium.com/@jennwv/nine-things-i-wish-i-had-known-the-first-time-i-came-to-nips-b939330661ed)

NeurIPS 2019 Schedule
[https://neurips.cc/Conferences/2019/Schedule?showEvent=13169](https://neurips.cc/Conferences/2019/Schedule?showEvent=13169)

Key trends from NeurIPS 2019
[https://huyenchip.com/2019/12/18/key-trends-neurips-2019.html](https://huyenchip.com/2019/12/18/key-trends-neurips-2019.html)




-----


## 20200115

krishna2.github.io/notes.md at master ¬∑ krishna2/krishna2.github.io

https://github.com/krishna2/krishna2.github.io/blob/master/ml/notes.md

tensorflow/nmt: TensorFlow Neural Machine Translation Tutorial

https://github.com/tensorflow/nmt

The Illustrated Transformer ‚Äì Jay Alammar ‚Äì Visualizing machine learning one concept at a time

http://jalammar.github.io/illustrated-transformer/

Tensor2Tensor Intro - Colaboratory

https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb#scrollTo=s19ucTii_wYb

Kullback-Leibler Divergence Explained ‚Äî Count Bayesie

https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained

Google AI Blog: Transformer: A Novel Neural Network Architecture for Language Understanding

https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html

(157) Attention is all you need; Attentional Neural Network Models | ≈Åukasz Kaiser | Masterclass - YouTube

https://www.youtube.com/watch?v=rBCqOTEfxvg

Attention Is All You Need - YouTube

https://www.youtube.com/watch?v=iDulhoQ2pro

Language Learning with BERT - TensorFlow and Deep Learning Singapore - YouTube

https://www.youtube.com/watch?v=0EtD5ybnh_s

Attention in Neural Networks - YouTube

https://www.youtube.com/watch?v=W2rWgXJBZhU

[Transformer] Attention Is All You Need | AISC Foundational - YouTube

https://www.youtube.com/watch?v=S0KakHcj_rs

Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) - YouTube

https://www.youtube.com/watch?v=WCUNPb-5EYI&t=405s

Transformer Networks - How to Roll Your Own Google Translate - YouTube

https://www.youtube.com/watch?v=1gXMkKBYgyI

Deep Learning 7. Attention and Memory in Deep Learning - YouTube

https://www.youtube.com/watch?v=Q57rzaHHO0k

Applying the four step "Embed, Encode, Attend, Predict" framework to predict document similarity - YouTube

https://www.youtube.com/watch?v=HfnjQIhQzME

[NUS CS6101 Deep Learning for NLP] S8 - Transformer Networks and CNNs - YouTube

https://www.youtube.com/watch?v=yCdl2afW88k

Lukasz Kaiser at AI Frontiers 2017: One Model to Learn It All - YouTube

https://www.youtube.com/watch?v=8FpdEmySsuc

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - YouTube

https://www.youtube.com/watch?v=-9evrZnBorM

CMU Neural Nets for NLP 2017 (9): Attention - YouTube

https://www.youtube.com/watch?v=MhTMgFvoaD8

A friendly introduction to Recurrent Neural Networks - YouTube

https://www.youtube.com/watch?v=UNmqTiOnRfg&t=9s

ROC and AUC, Clearly Explained! - YouTube

https://www.youtube.com/watch?v=4jRBRDbJemM

TensorFlow 2.0 Full Tutorial - Python Neural Networks for Beginners - YouTube

https://www.youtube.com/watch?v=6g4O5UOH304

Concentration Programming Music - YouTube

https://www.youtube.com/watch?v=0r6C3z3TEKw

ai.bythebay.io: Stephen Merity, Attention and Memory in Deep Learning Networks - YouTube

https://www.youtube.com/watch?v=uuPZFWJ-4bE

Lecture 10: Neural Machine Translation and Models with Attention - YouTube

https://www.youtube.com/watch?v=IxQtK2SjWWM

Sequence to Sequence Learning with Encoder-Decoder Neural Network Models by Dr. Ananth Sankar - YouTube

https://www.youtube.com/watch?v=bBBYPuVUnug

Lecture 8 - Generating Language with Attention [Chris Dyer] - YouTube

https://www.youtube.com/watch?v=ah7_mfl7LD0

Attention Mechanisms in Recurrent Neural Networks (RNNs) - IGGG - YouTube

https://www.youtube.com/watch?v=QuvRWevJMZ4

Deep learning Google TensorFlow workshop | ≈Åukasz Kaiser | Masterclass - YouTube

https://www.youtube.com/watch?v=XQQQbe3xN5o

Kaggle Reading Group: Attention is All You Need | Kaggle - YouTube

https://www.youtube.com/watch?v=54uLU7Nxyv8

PR-049: Attention is All You Need - YouTube

https://www.youtube.com/watch?v=6zGgVIlStXs

Deep Learning: A Crash Course - YouTube

https://www.youtube.com/watch?v=r0Ogt-q956I

Google AI Blog: Accelerating Deep Learning Research with the Tensor2Tensor Library

https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html

Tensor2Tensor Intro - Colaboratory

https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb

tensorflow/tensor2tensor: Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research.

https://github.com/tensorflow/tensor2tensor

[1706.03059] Depthwise Separable Convolutions for Neural Machine Translation

https://arxiv.org/abs/1706.03059

[1706.05137] One Model To Learn Them All

https://arxiv.org/abs/1706.05137

[1801.09797] Discrete Autoencoders for Sequence Models

https://arxiv.org/abs/1801.09797

[1801.10198] Generating Wikipedia by Summarizing Long Sequences

https://arxiv.org/abs/1801.10198

[1802.05751] Image Transformer

https://arxiv.org/abs/1802.05751

[1804.00247] Training Tips for the Transformer Model

https://arxiv.org/abs/1804.00247

[1803.02155] Self-Attention with Relative Position Representations

https://arxiv.org/abs/1803.02155

[1803.03382] Fast Decoding in Sequence Models using Discrete Latent Variables

https://arxiv.org/abs/1803.03382

[1804.04235] Adafactor: Adaptive Learning Rates with Sublinear Memory Cost

https://arxiv.org/abs/1804.04235

The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) ‚Äì Jay Alammar ‚Äì Visualizing machine learning one concept at a time

http://jalammar.github.io/illustrated-bert/

The Illustrated GPT-2 (Visualizing Transformer Language Models) ‚Äì Jay Alammar ‚Äì Visualizing machine learning one concept at a time

http://jalammar.github.io/illustrated-gpt2/

Visual Information Theory -- colah's blog

https://colah.github.io/posts/2015-09-Visual-Information/

Deep Learning, NLP, and Representations - colah's blog

http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/

Understanding LSTM Networks -- colah's blog

http://colah.github.io/posts/2015-08-Understanding-LSTMs/

Neural Networks, Types, and Functional Programming -- colah's blog

https://colah.github.io/posts/2015-09-NN-Types-FP/

Understanding LSTM Networks -- colah's blog

http://colah.github.io/posts/2015-08-Understanding-LSTMs/

On word embeddings - Part 1

https://ruder.io/word-embeddings-1/index.html

Approximating the Softmax for Learning Word Embeddings

https://ruder.io/word-embeddings-softmax/

On word embeddings - Part 3: The secret ingredients of word2vec

https://ruder.io/secret-word2vec/index.html

A survey of cross-lingual word embedding models

https://ruder.io/cross-lingual-embeddings/index.html

Word embeddings in 2017: Trends and future directions

https://ruder.io/word-embeddings-2017/index.html

Deep Learning for NLP Best Practices

https://ruder.io/deep-learning-nlp-best-practices/index.html#attention

The State of Transfer Learning in NLP

https://ruder.io/state-of-transfer-learning-in-nlp/

BERT Word Embeddings Tutorial ¬∑ Chris McCormick

https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/

BERT Fine-Tuning Tutorial with PyTorch ¬∑ Chris McCormick

https://mccormickml.com/2019/07/22/BERT-fine-tuning/

The Annotated Transformer

https://nlp.seas.harvard.edu/2018/04/03/attention.html

The Annotated Encoder Decoder | A PyTorch tutorial implementing Bahdanau et al. (2015)

https://bastings.github.io/annotated_encoder_decoder/

Copy of BERT_lab.ipynb - Colaboratory

https://colab.research.google.com/drive/1-UdelAqtD_2KrUIV_NTL59kgqo8m1Y2H#scrollTo=lJLcUBF5GOyU

BERT Explained: A Complete Guide with Theory and Tutorial ‚Äì Towards Machine Learning

https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/

Transfer Learning in NLP - Modern NLP - Medium

https://medium.com/modern-nlp/transfer-learning-in-nlp-f5035cc3f62f

Attn: Illustrated Attention - Towards Data Science

https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3

NLP: Contextualized word embeddings from BERT - Towards Data Science

https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b

Deconstructing BERT - Towards Data Science

https://towardsdatascience.com/deconstructing-bert-reveals-clues-to-its-state-of-art-performance-in-nlp-tasks-76a7e828c0f1

BERT Classifier: Just Another Pytorch Model - Towards Data Science

https://towardsdatascience.com/bert-classifier-just-another-pytorch-model-881b3cf05784

bollu/bollu.github.io: code + contents of my website, and programming life

https://github.com/bollu/bollu.github.io#everything-you-know-about-word2vec-is-wrong

Transformers from scratch | Peter Bloem

http://www.peterbloem.nl/blog/transformers

Animated RNN, LSTM and GRU - Towards Data Science

https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45

Attn: Illustrated Attention - Towards Data Science

https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3

Illustrated: Efficient Neural Architecture Search - Towards Data Science

https://towardsdatascience.com/illustrated-efficient-neural-architecture-search-5f7387f9fb6

Illustrated Guide to LSTM‚Äôs and GRU‚Äôs: A step by step explanation

https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21

Visualisation of embedding relations (word2vec, BERT)

https://towardsdatascience.com/visualisation-of-embedding-relations-word2vec-bert-64d695b7f36

Attention? Attention!

https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html

Why I Keep a Research Blog

http://gregorygundersen.com/blog/2020/01/12/why-research-blog/


