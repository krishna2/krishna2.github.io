# NOTES : RANDOM STUFF, LINKS

## 2020-01-14

Why I Keep a Research Blog
http://gregorygundersen.com/blog/2020/01/12/why-research-blog/

15848417.pdf
https://web.stanford.edu/class/cs224n/reports/default/15848417.pdf

huggingface/tokenizers: üí•Fast State-of-the-Art Tokenizers optimized for Research and Production
[https://github.com/huggingface/tokenizers](https://github.com/huggingface/tokenizers)

Free ‚ÄúLanguage Learning with Netflix‚Äù extension makes studying Japanese almost too¬†easy | SoraNews24 -Japan News-
[https://soranews24.com/2020/01/12/free-language-learning-with-netflix-extension-makes-studying-japanese-almost-too-easy/](https://soranews24.com/2020/01/12/free-language-learning-with-netflix-extension-makes-studying-japanese-almost-too-easy/)

jdevlin.pdf
[https://nlp.stanford.edu/seminar/details/jdevlin.pdf](https://nlp.stanford.edu/seminar/details/jdevlin.pdf)

Pre-training BERT from scratch with cloud TPU - Towards Data Science
[https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379](https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379)

Papers With Code : BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
[https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional](https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional)

BERT Fine-Tuning Tutorial with PyTorch ¬∑ Chris McCormick
[https://mccormickml.com/2019/07/22/BERT-fine-tuning/](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)

Fine-tuning Bert language model to get better results on text classification
[https://medium.com/analytics-vidhya/fine-tuning-bert-language-model-to-get-better-results-on-text-classification-3dac5e3c348e](https://medium.com/analytics-vidhya/fine-tuning-bert-language-model-to-get-better-results-on-text-classification-3dac5e3c348e)

BERT Explained: A Complete Guide with Theory and Tutorial ‚Äì Towards Machine Learning
[https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/)

BERT Technology introduced in 3-minutes ‚Äì mc.ai
[https://mc.ai/bert-technology-introduced-in-3-minutes/](https://mc.ai/bert-technology-introduced-in-3-minutes/)

Pretrained models ‚Äî transformers 2.3.0 documentation
[https://huggingface.co/transformers/pretrained_models.html](https://huggingface.co/transformers/pretrained_models.html)

Multi-lingual models ‚Äî transformers 2.3.0 documentation
[https://huggingface.co/transformers/multilingual.html](https://huggingface.co/transformers/multilingual.html)

Pre-training, Transformers, and Bi-directionality | AISC Blog
[https://aisc.ai.science/blog/2019/bert-pretraining-transformers-bidirectionality](https://aisc.ai.science/blog/2019/bert-pretraining-transformers-bidirectionality)

BERT Explained: State of the art language model for NLP
[https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)

BERT Technology introduced in 3-minutes - Towards Data Science
[https://towardsdatascience.com/bert-technology-introduced-in-3-minutes-2c2f9968268c](https://towardsdatascience.com/bert-technology-introduced-in-3-minutes-2c2f9968268c)

Paper Dissected: "Attention is All You Need" Explained | Machine Learning Explained
[http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/](http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/)

(155) Daniel Kahneman: Thinking Fast and Slow, Deep Learning, and AI | Artificial Intelligence Podcast - YouTube
[https://www.youtube.com/watch?v=UwwBG-MbniY&list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4](https://www.youtube.com/watch?v=UwwBG-MbniY&list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4)

Dissecting BERT Part 1: The Encoder - Dissecting BERT - Medium
[https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3](https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3)

Understanding BERT Part 2: BERT Specifics - Dissecting BERT - Medium
[https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73](https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73)

Dissecting BERT Appendix: The Decoder - Dissecting BERT - Medium
[https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f](https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f)

BERT in a Nutshell ‚Äì mc.ai
[https://mc.ai/bert-in-a-nutshell/](https://mc.ai/bert-in-a-nutshell/)

Understand Self-Attention in BERT Intuitively - Towards Data Science
[https://towardsdatascience.com/understand-self-attention-in-bert-intuitively-cd480cbff30b](https://towardsdatascience.com/understand-self-attention-in-bert-intuitively-cd480cbff30b)

Deconstructing BERT: Distilling 6 Patterns from 100 Million Parameters
[https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77](https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77)

Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention
[https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1)

3 subword algorithms help to improve your NLP model performance
[https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46](https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46)

What's the difference between wordpiece and sentencepiece?
[https://www.gitmemory.com/issue/google/sentencepiece/339/497563846](https://www.gitmemory.com/issue/google/sentencepiece/339/497563846)

A Deep Dive into Preprocessing in NLP | Machine Learning Explained
[https://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/](https://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/)

Learning to rank - Wikipedia
[https://en.wikipedia.org/wiki/Learning_to_rank](https://en.wikipedia.org/wiki/Learning_to_rank)

The first forum for newcomers to ML is co-located with NeurIPS, East Meeting Level 11,12, Vancouver Convention Center, Vancouver, BC, Canada, Monday, December 9th, 2019. | New In ML 2019
[https://nehzux.github.io/NewInML2019/](https://nehzux.github.io/NewInML2019/)

Nine things I wish I had known the first time I came to NIPS
[https://medium.com/@jennwv/nine-things-i-wish-i-had-known-the-first-time-i-came-to-nips-b939330661ed](https://medium.com/@jennwv/nine-things-i-wish-i-had-known-the-first-time-i-came-to-nips-b939330661ed)

NeurIPS 2019 Schedule
[https://neurips.cc/Conferences/2019/Schedule?showEvent=13169](https://neurips.cc/Conferences/2019/Schedule?showEvent=13169)

Key trends from NeurIPS 2019
[https://huyenchip.com/2019/12/18/key-trends-neurips-2019.html](https://huyenchip.com/2019/12/18/key-trends-neurips-2019.html)




-----


