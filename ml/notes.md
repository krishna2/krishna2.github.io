# NOTES : RANDOM STUFF, LINKS

## 2020-01-13

Why I Keep a Research Blog
http://gregorygundersen.com/blog/2020/01/12/why-research-blog/

From Convolution to Neural Network
http://gregorygundersen.com/blog/2017/02/24/cnns/

Why Backprop Goes Backward
http://gregorygundersen.com/blog/2018/04/15/backprop/

The Reparameterization Trick
http://gregorygundersen.com/blog/2018/04/29/reparameterization/

An Example of Probabilistic Machine Learning
http://gregorygundersen.com/blog/2018/06/13/probabilistic-ml/

Dot Product: Equivalence of Definitions
http://gregorygundersen.com/blog/2018/06/26/dot-product/

Factor Analysis in Detail
http://gregorygundersen.com/blog/2018/08/08/factor-analysis/

The KL Divergence: From Information to Density Estimation
http://gregorygundersen.com/blog/2019/01/22/kld/

Random Noise and the Central Limit Theorem
http://gregorygundersen.com/blog/2019/02/01/clt/

huggingface/tokenizers: üí•Fast State-of-the-Art Tokenizers optimized for Research and Production
https://github.com/huggingface/tokenizers

Free ‚ÄúLanguage Learning with Netflix‚Äù extension makes studying Japanese almost too¬†easy | SoraNews24 -Japan News-
https://soranews24.com/2020/01/12/free-language-learning-with-netflix-extension-makes-studying-japanese-almost-too-easy/

jdevlin.pdf
https://nlp.stanford.edu/seminar/details/jdevlin.pdf

Pre-training BERT from scratch with cloud TPU - Towards Data Science
https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379

Papers With Code : BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional

BERT Fine-Tuning Tutorial with PyTorch ¬∑ Chris McCormick
https://mccormickml.com/2019/07/22/BERT-fine-tuning/

Fine-tuning Bert language model to get better results on text classification
https://medium.com/analytics-vidhya/fine-tuning-bert-language-model-to-get-better-results-on-text-classification-3dac5e3c348e

15848417.pdf
https://web.stanford.edu/class/cs224n/reports/default/15848417.pdf

BERT Explained: A Complete Guide with Theory and Tutorial ‚Äì Towards Machine Learning
https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/

BERT Technology introduced in 3-minutes ‚Äì mc.ai
https://mc.ai/bert-technology-introduced-in-3-minutes/

Pretrained models ‚Äî transformers 2.3.0 documentation
https://huggingface.co/transformers/pretrained_models.html

Multi-lingual models ‚Äî transformers 2.3.0 documentation
https://huggingface.co/transformers/multilingual.html

ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators | OpenReview
https://openreview.net/forum?id=r1xMH1BtvB

Pre-training, Transformers, and Bi-directionality | AISC Blog
https://aisc.ai.science/blog/2019/bert-pretraining-transformers-bidirectionality

BERT Explained: State of the art language model for NLP
https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270

BERT Technology introduced in 3-minutes - Towards Data Science
https://towardsdatascience.com/bert-technology-introduced-in-3-minutes-2c2f9968268c

-----


